{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d18e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76494e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb15d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Using cached huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.10.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Using cached transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "Downloading PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.5/167.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp311-cp311-macosx_11_0_arm64.whl (425 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.4/425.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Using cached filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: safetensors, regex, pyyaml, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.12.4 huggingface-hub-0.17.3 pyyaml-6.0.1 regex-2023.10.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16e5032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.3-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: absl-py in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from tensorflow-datasets) (2.0.0)\n",
      "Collecting array-record (from tensorflow-datasets)\n",
      "  Downloading array_record-0.4.1-py310-none-any.whl.metadata (503 bytes)\n",
      "Requirement already satisfied: click in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from tensorflow-datasets) (8.1.7)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-macosx_11_0_arm64.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting etils>=0.9.0 (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets)\n",
      "  Downloading etils-1.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from tensorflow-datasets) (1.26.0)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from tensorflow-datasets) (4.21.12)\n",
      "Requirement already satisfied: psutil in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from tensorflow-datasets) (2.31.0)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: termcolor in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from tensorflow-datasets) (2.3.0)\n",
      "Collecting toml (from tensorflow-datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tqdm (from tensorflow-datasets)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Collecting fsspec (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets)\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting importlib_resources (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets)\n",
      "  Downloading importlib_resources-6.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: typing_extensions in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (4.5.0)\n",
      "Requirement already satisfied: zipp in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow-datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in ./anaconda3/envs/testenv/lib/python3.11/site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Collecting absl-py (from tensorflow-datasets)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow-datasets)\n",
      "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf>=3.20 (from tensorflow-datasets)\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_datasets-4.9.3-py3-none-any.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.5.1-py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading array_record-0.4.1-py310-none-any.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Downloading importlib_resources-6.1.0-py3-none-any.whl (33 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21484 sha256=7649746d7e9bfabfc778c3eb445db61e09ca77eaf86f77f622a44facd65f3963\n",
      "  Stored in directory: /Users/hemalathat/Library/Caches/pip/wheels/90/74/b1/9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, tqdm, toml, protobuf, promise, importlib_resources, fsspec, etils, absl-py, googleapis-common-protos, tensorflow-metadata, array-record, tensorflow-datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.12\n",
      "    Uninstalling protobuf-4.21.12:\n",
      "      Successfully uninstalled protobuf-4.21.12\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.0.0\n",
      "    Uninstalling absl-py-2.0.0:\n",
      "      Successfully uninstalled absl-py-2.0.0\n",
      "Successfully installed absl-py-1.4.0 array-record-0.4.1 dm-tree-0.1.8 etils-1.5.1 fsspec-2023.10.0 googleapis-common-protos-1.61.0 importlib_resources-6.1.0 promise-2.3 protobuf-3.20.3 tensorflow-datasets-4.9.3 tensorflow-metadata-1.14.0 toml-0.10.2 tqdm-4.66.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c7c856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load('test req og.csv',split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "          as_supervised=True,with_info=True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851c8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901b541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review):\n",
    "    return tokenizer.encode_plus(review,\n",
    "                add_special_tokens = True, # add [CLS], [SEP]\n",
    "                max_length = max_length, # max length of the text that can go to BERT\n",
    "                pad_to_max_length = True, # add [PAD] tokens\n",
    "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9bacb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "389db4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d07471cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_examples(ds, limit=-1):\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "    for review, label in tfds.as_numpy(ds):\n",
    "        bert_input = convert_example_to_feature(review.decode())\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2191daec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 16:25:03.788145: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/hemalathat/anaconda3/envs/testenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with Training Dataset 58.91490578651428\n",
      "Done with Testing Dataset 59.07609510421753\n"
     ]
    }
   ],
   "source": [
    "# train dataset\n",
    "start=time.time()\n",
    "ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(batch_size)\n",
    "print(\"Done with Training Dataset\",time.time()-start)\n",
    "# test dataset\n",
    "start=time.time()\n",
    "ds_test_encoded = encode_examples(ds_test).batch(batch_size)\n",
    "print(\"Done with Testing Dataset\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bc16c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch, though multiple epochs might be better as long \n",
    "# as we will not overfit the model\n",
    "number_of_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e466a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|█████████| 440M/440M [00:14<00:00, 29.6MB/s]\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed866d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ace8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4167/4167 [==============================] - ETA: 0s - loss: 0.2424 - accuracy: 0.9004"
     ]
    }
   ],
   "source": [
    "bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447417e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"These earphones are too good. I love it\"\n",
    "\n",
    "predict_input = tokenizer.encode(test_sentence,truncation=True,padding=True,return_tensors=\"tf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
